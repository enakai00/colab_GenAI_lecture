{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Transformer recipe generator training\n",
        "\n",
        "Note: This notebook is designed to run with GPU runtime."
      ],
      "metadata": {
        "id": "Gw-Vj6BZFHOf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Install Huggingface libraries to use the pretrained tokenizer and the recipe dataset.\n",
        "\n",
        "**You can igore the error message like `ERROR: pip's dependency resolver does not currently take into account...`.**"
      ],
      "metadata": {
        "id": "aA1wNyoRFMYk"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "qR4fKhETJ_M7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cbfb832e-f7c4-4335-9ca2-c31c50559e05"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m491.5/491.5 kB\u001b[0m \u001b[31m30.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.6/193.6 kB\u001b[0m \u001b[31m20.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for transformers (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torch 2.6.0+cu124 requires nvidia-cublas-cu12==12.4.5.8; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cublas-cu12 12.5.3.2 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-cupti-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-cupti-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-nvrtc-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-nvrtc-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-runtime-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-runtime-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cudnn-cu12==9.1.0.70; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cudnn-cu12 9.3.0.75 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cufft-cu12==11.2.1.3; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cufft-cu12 11.2.3.61 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-curand-cu12==10.3.5.147; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-curand-cu12 10.3.6.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cusolver-cu12==11.6.1.9; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusolver-cu12 11.6.3.83 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cusparse-cu12==12.3.1.170; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusparse-cu12 12.5.1.3 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-nvjitlink-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-nvjitlink-cu12 12.5.82 which is incompatible.\n",
            "gcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2025.3.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "pip install -qU git+https://github.com/huggingface/transformers.git datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import modules and set random seeds."
      ],
      "metadata": {
        "id": "txDk8VABFM-i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os, random\n",
        "import numpy as np\n",
        "from pandas import DataFrame\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models, saving\n",
        "\n",
        "random.seed(20230629)\n",
        "np.random.seed(20230629)\n",
        "tf.random.set_seed(20230629)\n",
        "\n",
        "plt.rcParams.update({'font.size': 10})"
      ],
      "metadata": {
        "id": "nzi1HcNOKNQA"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Download the pretrained tokenizer and check the vacabulary size."
      ],
      "metadata": {
        "id": "iHYCLcksFUzM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoConfig\n",
        "model_ckpt = 'bert-base-uncased'\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_ckpt)\n",
        "VOCAB_SIZE = AutoConfig.from_pretrained(model_ckpt).vocab_size\n",
        "\n",
        "print(f'Vocabulary size: {VOCAB_SIZE}')"
      ],
      "metadata": {
        "id": "oswcYYV4Yle4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5ae0c838-c018-4f84-9eca-d0aedae135ac"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary size: 30522\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Download the recipe dataset and extract directions texts."
      ],
      "metadata": {
        "id": "WU0N7XV_FWtf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "recipe = load_dataset('Shengtao/recipe')\n",
        "\n",
        "def join_title_and_directions(title_directions):\n",
        "    title, directions = title_directions\n",
        "    return f'Recipe for {title}: {directions}'\n",
        "\n",
        "recipe_texts = zip(recipe['train']['title'], recipe['train']['directions'])\n",
        "recipe_texts = [*map(join_title_and_directions, recipe_texts)]\n",
        "recipe_texts = recipe_texts[::4] # Select 25% of the entire training set."
      ],
      "metadata": {
        "id": "rP1mzvDWFbBB"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create the training and test datasets, truncating long texts into 512 words."
      ],
      "metadata": {
        "id": "NYiSGJ3hFeDe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "MAX_LEN = 128\n",
        "train_set, test_set = train_test_split(recipe_texts, test_size=0.1)\n",
        "\n",
        "train_set = tokenizer(train_set, max_length=MAX_LEN,\n",
        "                      padding='max_length', truncation=True)\n",
        "train_text = np.array(train_set['input_ids'])[:, :-1]\n",
        "train_label = np.array(train_set['input_ids'])[:, 1:]\n",
        "\n",
        "test_set = tokenizer(test_set, max_length=MAX_LEN,\n",
        "                     padding='max_length', truncation=True)\n",
        "test_text = np.array(test_set['input_ids'])[:, :-1]\n",
        "test_label = np.array(test_set['input_ids'])[:, 1:]"
      ],
      "metadata": {
        "id": "LYaQaUF7YrT_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define the positional embedding layer."
      ],
      "metadata": {
        "id": "HZvEalaoFl3W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@saving.register_keras_serializable()\n",
        "class Embeddings(layers.Layer):\n",
        "    def __init__(self, max_len, vocab_size, embed_dim, **kwargs):\n",
        "        super(Embeddings, self).__init__(**kwargs)\n",
        "        self.max_len = max_len\n",
        "        self.vocab_size = vocab_size\n",
        "        self.embed_dim = embed_dim\n",
        "        self.token_emb = layers.Embedding(vocab_size, embed_dim)\n",
        "        self.pos_emb = layers.Embedding(max_len, embed_dim)\n",
        "        self.ln = layers.LayerNormalization(epsilon=1e-12)\n",
        "        self.dropout = layers.Dropout(rate=0.5)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        seq_len = tf.shape(inputs)[-1]\n",
        "        position_ids = tf.range(start=0, limit=seq_len, delta=1)\n",
        "        position_embeddings = self.pos_emb(position_ids)\n",
        "        token_embeddings = self.token_emb(inputs)\n",
        "\n",
        "        # Add positional embeddings\n",
        "        embeddings = token_embeddings + position_embeddings\n",
        "        embeddings = self.ln(embeddings)\n",
        "        embeddings = self.dropout(embeddings)\n",
        "        return embeddings\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        pass\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super().get_config()\n",
        "        config.update({\n",
        "            'max_len': self.max_len,\n",
        "            'vocab_size': self.vocab_size,\n",
        "            'embed_dim': self.embed_dim,\n",
        "        })\n",
        "        return config"
      ],
      "metadata": {
        "id": "rc0nsCCNYyPX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dfiner the Transformer encoder block."
      ],
      "metadata": {
        "id": "-C72nUW2GDf0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@saving.register_keras_serializable()\n",
        "class TransformerBlock(layers.Layer):\n",
        "    def __init__(self, num_heads, key_dim, embed_dim, ff_dim, **kwargs):\n",
        "        super(TransformerBlock, self).__init__(**kwargs)\n",
        "        self.num_heads = num_heads\n",
        "        self.key_dim = key_dim\n",
        "        self.embed_dim = embed_dim\n",
        "        self.ff_dim = ff_dim\n",
        "        self.attn = layers.MultiHeadAttention(\n",
        "            num_heads, key_dim, output_shape=embed_dim)\n",
        "        self.dropout_1 = layers.Dropout(rate=0.1)\n",
        "        self.ln_1 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.ffn_1 = layers.Dense(self.ff_dim, activation='relu')\n",
        "        self.ffn_2 = layers.Dense(self.embed_dim)\n",
        "        self.dropout_2 = layers.Dropout(rate=0.1)\n",
        "        self.ln_2 = layers.LayerNormalization(epsilon=1e-6)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        # Multi-head attention\n",
        "        attention_output, attention_scores = self.attn(\n",
        "            inputs, inputs, inputs, # Inputs for Query, Value, Key\n",
        "            use_causal_mask=True,\n",
        "            return_attention_scores=True)\n",
        "        attention_output = self.dropout_1(attention_output)\n",
        "        attention_output = attention_output + inputs # Skip connection\n",
        "        attention_output = self.ln_1(attention_output)\n",
        "\n",
        "        # Feed forward\n",
        "        ffn_1 = self.ffn_1(attention_output)\n",
        "        ffn_2 = self.ffn_2(ffn_1)\n",
        "        ffn_output = self.dropout_2(ffn_2)\n",
        "        ffn_output = ffn_output + attention_output # Skip connection\n",
        "        ffn_output = self.ln_2(ffn_output)\n",
        "        return (ffn_output, attention_scores)\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        pass\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super().get_config()\n",
        "        config.update({\n",
        "                \"key_dim\": self.key_dim,\n",
        "                \"embed_dim\": self.embed_dim,\n",
        "                \"num_heads\": self.num_heads,\n",
        "                \"ff_dim\": self.ff_dim,\n",
        "        })\n",
        "        return config"
      ],
      "metadata": {
        "id": "TD8MmnKHYzKW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define the Transformer model consisted of a single Transformer encoder blocks."
      ],
      "metadata": {
        "id": "Hse3GMecGIMi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "EMBEDDING_DIM = 512\n",
        "N_HEADS = 4\n",
        "KEY_DIM = EMBEDDING_DIM // N_HEADS\n",
        "FEED_FORWARD_DIM = 2048\n",
        "\n",
        "text_inputs = layers.Input(shape=(None,), dtype=tf.int32, name='input_ids')\n",
        "x = Embeddings(\n",
        "    MAX_LEN, VOCAB_SIZE, EMBEDDING_DIM, name='text_embedding')(text_inputs)\n",
        "x, attention_scores = TransformerBlock(\n",
        "    N_HEADS, KEY_DIM, EMBEDDING_DIM, FEED_FORWARD_DIM, name='Transformer')(x)\n",
        "output = layers.Dense(VOCAB_SIZE, activation='softmax', name='softmax')(x)\n",
        "\n",
        "# Model for training and prediction\n",
        "transformer_model = models.Model(\n",
        "    inputs=text_inputs, outputs=output,\n",
        "    name='Transformer_next_word_predictor')\n",
        "\n",
        "# Model for inference including attention scores\n",
        "attention_scores_model = models.Model(\n",
        "    inputs=text_inputs,\n",
        "    outputs=[output, attention_scores],\n",
        "    name='Transformer_attention_scores')\n",
        "\n",
        "transformer_model.summary()"
      ],
      "metadata": {
        "id": "wykIJP5VZpCM",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 273
        },
        "outputId": "577e0a05-c01f-4e8a-b5b3-319daa10c750"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"Transformer_next_word_predictor\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"Transformer_next_word_predictor\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ input_ids (\u001b[38;5;33mInputLayer\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)                │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ text_embedding (\u001b[38;5;33mEmbeddings\u001b[0m)          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)           │      \u001b[38;5;34m15,693,824\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ Transformer (\u001b[38;5;33mTransformerBlock\u001b[0m)       │ [(\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m), (\u001b[38;5;45mNone\u001b[0m,  │       \u001b[38;5;34m3,152,384\u001b[0m │\n",
              "│                                      │ \u001b[38;5;34m4\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)]             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ softmax (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30522\u001b[0m)         │      \u001b[38;5;34m15,657,786\u001b[0m │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ input_ids (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)                │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ text_embedding (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embeddings</span>)          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)           │      <span style=\"color: #00af00; text-decoration-color: #00af00\">15,693,824</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ Transformer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TransformerBlock</span>)       │ [(<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>), (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>,  │       <span style=\"color: #00af00; text-decoration-color: #00af00\">3,152,384</span> │\n",
              "│                                      │ <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)]             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ softmax (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30522</span>)         │      <span style=\"color: #00af00; text-decoration-color: #00af00\">15,657,786</span> │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m34,503,994\u001b[0m (131.62 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">34,503,994</span> (131.62 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m34,503,994\u001b[0m (131.62 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">34,503,994</span> (131.62 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Compile the model using the Adam optimizer, and the sparse categorical crossentroy as a loss function."
      ],
      "metadata": {
        "id": "GrE_7trZGSup"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "transformer_model.compile('adam',\n",
        "                          loss='sparse_categorical_crossentropy',\n",
        "                          metrics=['acc'])"
      ],
      "metadata": {
        "id": "wo38ilacaDYm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train the model."
      ],
      "metadata": {
        "id": "mXGLCB7gGW45"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "history = transformer_model.fit(train_text, train_label,\n",
        "                                validation_data=(test_text, test_label),\n",
        "                                batch_size=8, epochs=10)"
      ],
      "metadata": {
        "id": "zM3-QplLayeu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8f15608f-d9ef-4e02-cd6c-28328f4c2374"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m921/921\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 56ms/step - acc: 0.3965 - loss: 3.5751 - val_acc: 0.5369 - val_loss: 2.2784\n",
            "Epoch 2/10\n",
            "\u001b[1m921/921\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m68s\u001b[0m 48ms/step - acc: 0.5330 - loss: 2.2337 - val_acc: 0.5721 - val_loss: 2.0532\n",
            "Epoch 3/10\n",
            "\u001b[1m921/921\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 47ms/step - acc: 0.5637 - loss: 1.9997 - val_acc: 0.5854 - val_loss: 1.9524\n",
            "Epoch 4/10\n",
            "\u001b[1m921/921\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 47ms/step - acc: 0.5805 - loss: 1.8706 - val_acc: 0.5959 - val_loss: 1.9001\n",
            "Epoch 5/10\n",
            "\u001b[1m921/921\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 47ms/step - acc: 0.5919 - loss: 1.7796 - val_acc: 0.6008 - val_loss: 1.8692\n",
            "Epoch 6/10\n",
            "\u001b[1m921/921\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 47ms/step - acc: 0.6011 - loss: 1.7133 - val_acc: 0.6035 - val_loss: 1.8530\n",
            "Epoch 7/10\n",
            "\u001b[1m921/921\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 48ms/step - acc: 0.6082 - loss: 1.6599 - val_acc: 0.6080 - val_loss: 1.8363\n",
            "Epoch 8/10\n",
            "\u001b[1m921/921\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 48ms/step - acc: 0.6145 - loss: 1.6167 - val_acc: 0.6085 - val_loss: 1.8312\n",
            "Epoch 9/10\n",
            "\u001b[1m921/921\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m84s\u001b[0m 50ms/step - acc: 0.6192 - loss: 1.5786 - val_acc: 0.6109 - val_loss: 1.8311\n",
            "Epoch 10/10\n",
            "\u001b[1m921/921\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 47ms/step - acc: 0.6245 - loss: 1.5445 - val_acc: 0.6121 - val_loss: 1.8335\n",
            "CPU times: user 1min 29s, sys: 21.1 s, total: 1min 51s\n",
            "Wall time: 11min 51s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Mount google drive and save the trained model."
      ],
      "metadata": {
        "id": "jLuTMwFSGbn0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "model_file = '/content/gdrive/My Drive/Transformer_recipe_generator.keras'\n",
        "transformer_model.save(model_file)\n",
        "!ls -lh '{model_file}'\n",
        "\n",
        "model_file = '/content/gdrive/My Drive/Transformer_attention_scores.keras'\n",
        "attention_scores_model.save(model_file)\n",
        "!ls -lh '{model_file}'"
      ],
      "metadata": {
        "id": "dDMKp333d33R",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5252b1a9-9a78-4cde-f0f0-37e0f8bc4f27"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n",
            "-rw------- 1 root root 395M Feb  4 23:25 '/content/gdrive/My Drive/Transformer_recipe_generator.keras'\n",
            "-rw------- 1 root root 132M Feb  4 23:26 '/content/gdrive/My Drive/Transformer_attention_scores.keras'\n"
          ]
        }
      ]
    }
  ]
}
